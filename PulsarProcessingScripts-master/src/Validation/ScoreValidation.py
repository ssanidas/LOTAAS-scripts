"""
Script which validates the scores generated by the ScoreGenerator.py script. Directly
compares the newly generated scores with those from the original test_22.py script.

Calculates some basic statistics on the results and summarises them. This validation
script should be used to give an indication of whether or not changes made to the score
generation script have worked.

Rob Lyon <robert.lyon@cs.man.ac.uk>

+-----------------------------------------------------------------------------------------+
+                       PLEASE RECORD ANY MODIFICATIONS YOU MAKE BELOW                    +
+-----------------------------------------------------------------------------------------+
+ Revision |   Author    | Description                                       |    DATE    +
+-----------------------------------------------------------------------------------------+
 Revision:0    Rob Lyon    Initial version of the code                         03/02/2014

"""

# Standard library Imports:
import glob,os,sys

# Custom file Imports:
import Utilities
import Candidate
import ScoreWatcher

# Command Line processing Imports:
from optparse import OptionParser

# ******************************
#
# CLASS DEFINITION
#
# ******************************

class ScoreValidation(Utilities.Utilities):
    """                
    Compares the scores created by ScoreGenerator.py to the scores originally created
    by test_22.py .
    
    """
    
    # ******************************
    #
    # Constructor.
    #
    # ******************************
    
    def __init__(self,debugFlag):
        
        Utilities.Utilities.__init__(self, debugFlag)
        self.scoreScount = 22 # The total number of scores we should expect, 22 by default.
        self.metaFile = ""
        self.epsilon = 0.000005 # Used during score comparison.
        self.MLDataPath = "NewLabelledData.arff"
        self.MLDataPosPath = "NewLabelledDataPositives.arff"
        self.MLDataNegPath = "NewLabelledDataNegatives.arff"
        
    # ******************************
    #
    # MAIN METHOD AND ENTRY POINT.
    #
    # ******************************

    def main(self,argv=None):
        """
        Main entry point for the Application. Processes command line
        input and begins creating the scores.
        
        Parameters:
        
        originalScoresFile    -    The path to the file containing data on the original value
                                   of the scores.
        """
        
        # Python 2.4 argument processing.
        parser = OptionParser()

        # REQUIRED ARGUMENTS
        parser.add_option('-m', action="store", dest="path_meta",type="string",help='The path to the meta data file (required).',default="")

        # OPTIONAL ARGUMENTS
        parser.add_option('-s', action="store", dest="scores",type="int",help='The number of scores being used (optional).',default=22)
        parser.add_option("-l", action="store_true", dest="MLData",help='Machine learning data flag - will write out a file of labeled data (optional).',default=False)

        (args,options) = parser.parse_args()# @UnusedVariable : Comment for IDE to ignore warning.
        
        # Update variables with command line parameters.
        self.metaFile = args.path_meta
        self.scoreScount=args.scores
        self.getMLData=args.MLData
        
        print "\n************************************"
        print "| Validating score generation code |"
        print "************************************"
        
        # Check a validation file has been supplied. This file should have a 
        # very specific format. It should contain the unique name for each candidate,
        # its true class label (i.e. if a candidate is a definite pulsar its true class
        # label would be "POSITIVE", else its label is "NEGATIVE". The file should also
        # contain details of why each candidate is unique, i.e. if one of its scores is
        # unusual which could mean it is the highest or the lowest of all those observed.
        # 
        # Format:
        #
        # < Candidate File Name (no path) > , < True label > , < Unusual Score > , < Why score is unusual> , < Score 1 > , ... , < Score n>
        #
        # Example of the format (if only 5 scores):
        #
        #                                                            Why unusual   Score 4 
        #                                                                |             |
        #                                                                |   Score 2   |
        #                                                                |      |      |
        #                                                                v      v      v
        # 2008-05-11-05:24:10.01.fil_sigproc_001.phcx.gz.dat,NEGATIVE,1,MAX,100,4,56,0.01,99
        # 2008-06-12-06:33:19.03.fil_sigproc_002.phcx.gz.dat,POSITIVE,1,MIN,0.1,5,59,0.03,88
        # 2009-07-13-07:44:54.06.fil_sigproc_005.phcx.gz.dat,POSITIVE,3,MIN,33 ,6,33,0.06,78
        # 2009-08-14-08:16:23.05.fil_sigproc_006.phcx.gz.dat,NEGATIVE,4,MAX,37 ,7,89,2000,56
        # 2010-09-15-09:20:35.02.fil_sigproc_009.phcx.gz.dat,NEGATIVE,5,MIN,56 ,8,76,1.34,50
        # 2010-10-16-10:03:31.01.fil_sigproc_004.phcx.gz.dat,NEGATIVE,2,MAX,76 ,9,50,4.01,55
        #                            ^                           ^    ^      ^    ^        ^
        #                            |                           |    |      |    |        |
        #                        Filename                      Label  |  Score 1  |     Score 5   
        #                                                             |           |        
        #                                                      Unusual Score   Score 3        

        if(self.metaFile == ""):
            print "Exiting - no candidate meta data file provided to validate against."
            sys.exit(0)
            
        candidatesProcessed = 0;
        successes = 0;
        failures = 0;
        newCandidates = {}
        
        # First grab the new score data generated for each candidate.
        for cand in glob.glob('*.dat'):
            
            candidatesProcessed+=1
            
            try:
                print "Processing candidate:\t" , cand
                newCandidate=self.loadNewCandidateData(cand)
                
                # Add to the dictionary
                newCandidates[newCandidate.getPath()] = newCandidate
                
            except Exception as e: # catch *all* exceptions
                print "Error reading candidate score data:\n\t", sys.exc_info()[0]
                print self.format_exception(e)
                print cand, " did not have scores generated."
                failures+=1
                continue
                 
            successes+=1
            
        print "\nNumber of candidates found: ", len(newCandidates)
        
        if(len(newCandidates)==0):
            print "No individual candidate files found, re-run ant script using build that writes candidates to separate files."
        
        # Now we obtain old candidate data
        
        print "\nProcessing old candidate data for comparison.\n"
        oldCandidates = self.loadOldCandidateData(self.metaFile)
        print "Number of old candidates found: ", len(oldCandidates)
        
        if(len(newCandidates)>0):
            
            self.goCompare(oldCandidates, newCandidates)
            
            # Produce ML labelled data for further analysis if required.
            if(self.getMLData):
                print "Producing labeled machine learning data in ARFF format."
                self.produceMLData(oldCandidates, newCandidates)
        else:
            print "No candidates to compare!"
        
        print "\nCandidates processed:\t",candidatesProcessed
        print "Successes:\t", successes
        print "Failures:\t", failures
        print "Done."
        
    # ******************************
    #
    # Candidate data load functions.
    #
    # ******************************
    
    def loadNewCandidateData(self,fileName):
        """
        Method that reads candidate paths in from a file.
        
        Parameters:
        fileName    -    the string path to the file to load.
        
        Returns:
        
        An individual Candidate.py object from the data in the file.
        
        """
        
        lines = []
        
        # Here we read the .dat file which contains the scores. The scores
        # are all on a single line of the file, separated by a single space.
        if(self.fileExists(fileName)==True):
            
            f = open(fileName,'rU') # Read only access
    
            for line in f.readlines():
                    lines.append(line.replace("\n",""))
        
            f.close()
        
        filePath = str(os.getcwd()+"/"+fileName)
        candidate = Candidate.Candidate(fileName,filePath)
        
        # Add these conditions so that when testing, if not all scores are
        # completed the code can still execute without failing. First
        # check is for the case when the file is completely empty.
        if(len(lines)<0 or len(lines)==0):
            
            tempLine = ""
            for n in range(1, self.scoreScount+1):# @UnusedVariable : Comment for IDE to ignore warning.
                tempLine+="0.0,"
            
            lines.append(tempLine)
            
        # The second check is for when some but not all of the scores
        # have been read in i.e. 1-21 or some number in between.
        if(len(lines)==1):
            components=lines[0].split(",")
            if(len(components) != self.scoreScount):
                # We add ghost scores so that validation code can correctly execute.
                tempLine = ""
                for n in range(len(components), self.scoreScount+1):  # @UnusedVariable : Comment for IDE to ignore warning.
                    tempLine+=",0.0"
                lines[0] += tempLine
                
        # Finally add the scores to the candidate.        
        candidate.addScores(lines[0])
                
        return candidate
    
    # ******************************************************************************************
    
    def loadOldCandidateData(self,fileName):
        """
        Method that reads candidate paths in from a file.
        
        Parameters:
        fileName    -    the string path to the file to load.
        
        Returns:
        
        A list of individual Candidate.py objects from the data in the files.
        """
        
        oldCandidates = {}
        
        if(self.fileExists(fileName)==True):
            
            f = open(fileName,',"rU"') # Read only access
            
            # Process each candidate in the file.
            for line in f.readlines():

                contents=line.split(",")
                name = contents[0]
                label = contents[1]
                specialScore = contents[2]
                specialDescription = contents[3]
                scores = contents[4:]
                
                filePath = str(os.getcwd()+"/"+name)
                candidate = Candidate.Candidate(name,filePath)
                candidate.setScores(scores)
                candidate.setLabel(label)
                candidate.setSpecial(specialDescription)
                candidate.setSpecialScore(specialScore)
                oldCandidates[candidate.getPath()] = candidate             
                
            f.close()
               
        return oldCandidates
    
    # ******************************************************************************************
    
    def goCompare(self,oldCands,newCands):
        """
        Method that compares the old and new candidates, and outputs information
        that tells us how the scores generated for the new candidates differ from those
        for the old candidates.
        
        Parameters:
        oldCands    -    A list of individual Candidate.py objects from the data in the files.
        newCands    -    A list of individual Candidate.py objects from the data in the files.
        
        Returns:
        
        N/A
        
        """
        
        print "Running Comparison"
         
        # Create the objects that watch the scores being generated.
        watchers={}
        for n in range(1, self.scoreScount+1):

            watchers[n]= ScoreWatcher.ScoreWatcher(n,self.epsilon)

        # For each candidate...
        for newKey in newCands:
            
            # For each new candidate
            newCandidate = newCands[newKey]
            
            #  Get the key to its equivalent old candidate
            oldKey = newCandidate.getPath()
            
            # Obtain the old candidate.
            oldCandidate = oldCands[oldKey]
            
            specialScore=oldCandidate.getSpecialScore()
            specialDescription=oldCandidate.getSpecial()
            label=oldCandidate.getLabel()
            
            # For each score
            for n in range(1, self.scoreScount+1):
                
                # Get the score values.
                oldScore = oldCandidate.getScore(n)
                newScore = newCandidate.getScore(n)
                
                # Update the watcher based on these values.
                watcher= watchers[n]
                watcher.update(oldScore,newScore,label)
                
                # If this score is known to be important for this candidate,
                # i.e. it is known to have the max or minimum value observed.
                if(n==specialScore):
                    
                    # Get the short name of the candidate.
                    name=str(oldCandidate.getName())
                    messageStart = "\tScore " + str(n) + " for Candidate: " + name + " ( Class " + label + ") " # Start building an output message.
                    optimalScoreValue=watcher.getOptimalScoreDescription(n)
                    messageEnd = " " + str(optimalScoreValue) + " values desired."
                    
                    if(specialDescription=="MAX"): # If it is special because is was a max value.
                        
                        if(watcher.isEqual(newScore,oldScore,self.epsilon) == -1):
                            messageMid= "now LOWER ( " + str(newScore) + " < " + str(oldScore) + " ) than old score (was MAX)." 
                            message = messageStart + messageMid + messageEnd
                            watcher.addMessage(message)
                            
                        elif(watcher.isEqual(newScore,oldScore,self.epsilon) == 1):
                            messageMid= "now HIGHER ( " + str(newScore) + " > " + str(oldScore) + " ) than old score (was MAX)." 
                            message = messageStart + messageMid + messageEnd
                            watcher.addMessage(message)
                            
                        else:
                            messageMid= "now EQUAL ( " + str(newScore) + " = " + str(oldScore) + " ) to old score (was MAX)."  
                            message = messageStart + messageMid + messageEnd
                            watcher.addMessage(message)
                            
                    elif(specialDescription=="MIN"): # If it is special because is was a minimum value.
                        
                        if(watcher.isEqual(newScore,oldScore,self.epsilon) == -1):
                            messageMid= "now LOWER ( " + str(newScore) + " < " + str(oldScore) + " ) than old score (was MAX)." 
                            message = messageStart + messageMid + messageEnd
                            watcher.addMessage(message)
                            
                        elif(watcher.isEqual(newScore,oldScore,self.epsilon) == 1):
                            messageMid= "now HIGHER ( " + str(newScore) + " > " + str(oldScore) + " ) than old score (was MAX)." 
                            message = messageStart + messageMid + messageEnd
                            watcher.addMessage(message)
                            
                        else:
                            messageMid= "now EQUAL ( " + str(newScore) + " = " + str(oldScore) + " ) to old score (was MAX)."  
                            message = messageStart + messageMid + messageEnd
                            watcher.addMessage(message)
        
        # Force watchers to write out the information they have collected.            
        for n in range(1, self.scoreScount+1):
            watcher= watchers[n]
            print watcher.__str__()
            
        
    # ******************************************************************************************
    
    def produceMLData(self,oldCands,newCands):
        """
        Method that creates a WEKA compatible machine learning ARFF file for further analysis.
        
        Parameters:
        oldCands    -    A list of individual Candidate.py objects from the data in the files.
        newCands    -    A list of individual Candidate.py objects from the data in the files.
        
        Returns:  
        True if the ML data is produced, else False.
        
        """
        
        self.prepareARFFFile(self.MLDataPath)
        self.prepareARFFFile(self.MLDataPosPath)
        self.prepareARFFFile(self.MLDataNegPath)
        
        if(not self.fileExists(self.MLDataPath)):
            print "Machine learning labeled data file could not be created!"
            return False
        
        # For each candidate...
        for newKey in newCands:
            
            # For each new candidate
            newCandidate = newCands[newKey]
            
            #  Get the key to its equivalent old candidate
            oldKey = newCandidate.getPath()
            
            # Obtain the old candidate.
            oldCandidate = oldCands[oldKey]
            
            label=oldCandidate.getLabel()
            path = newCandidate.getPath()
            
            scoreLine="%"+path+"\n"
            
            # Change label to binary value for ML classifiers.
            if(label=="POSITIVE"):
                label="1"
            else:
                label="0"
            
            # For each score
            for n in range(1, self.scoreScount+1):
                
                # Get the score values.
                #oldScore = oldCandidate.getScore(n)
                newScore = newCandidate.getScore(n)
                
                if(n==1):
                    scoreLine+=str(newScore)
                else:
                    scoreLine+=","+str(newScore)
                    
            scoreLine+=","+label+"\n"
            
            self.appendToFile(self.MLDataPath, scoreLine)
            
            if(label=="1"):
                self.appendToFile(self.MLDataPosPath, scoreLine)
            else:
                self.appendToFile(self.MLDataNegPath, scoreLine)
            
        return True
                
    
    # ******************************************************************************************
    
    def prepareARFFFile(self,path):
        """
        Creates an ARFF file with the appropriate headers, reader for data
        to be written to the file.
        
        Parameters:
        path    -    the path to the file to prepare
        
        Returns:
        N/A
        """
        
        header = "@relation LabelledPulsarCandidates\n"
        
        for n in range(1, self.scoreScount+1):
            header += "@attribute Score"
            header += str(n)
            header += " numeric\n"
        
        header += "@attribute class {0,1}\n@data\n"
            
        self.appendToFile(path, header)
        
if __name__ == '__main__':
    ScoreValidation(True).main()